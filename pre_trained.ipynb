{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_lifter_dir = 'non shop lifters'\n",
    "lifter_dir = 'shop lifters'\n",
    "DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_lifter_df = pd.DataFrame(columns=['Video','Frames','FPS','Width','Height','Shoplifter'])\n",
    "lifter_df = pd.DataFrame(columns=['Video','Frames','FPS','Width','Height','Shoplifter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] : Load all the images.....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(531, 324)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"[INFO] : Load all the images.....\")\n",
    "non_lifter_dir = os.path.join(DIR, non_lifter_dir)\n",
    "lifetr_dir = os.path.join(DIR, lifter_dir)\n",
    "non_lifter_videos = glob.glob(non_lifter_dir + '*/*.mp4')\n",
    "lifter_videos = glob.glob(lifter_dir + '*/*.mp4')\n",
    "len(non_lifter_videos), len(lifter_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total videos in 'f:\\Cellula Internship cv\\Shop DataSet\\non shop lifters' before filtering: 531\n",
      "Total videos in 'f:\\Cellula Internship cv\\Shop DataSet\\non shop lifters' after filtering: 313\n",
      "\n",
      "Total videos in 'f:\\Cellula Internship cv\\Shop DataSet\\shop lifters' before filtering: 324\n",
      "Total videos in 'f:\\Cellula Internship cv\\Shop DataSet\\shop lifters' after filtering: 324\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.getcwd()\n",
    "\n",
    "non_shop_lifters_path = os.path.join(dataset_path, \"non shop lifters\")\n",
    "shop_lifters_path = os.path.join(dataset_path, \"shop lifters\")\n",
    "\n",
    "def get_unique_videos(folder_path, underscore_count):\n",
    "    \"\"\"Retrieve video files, filtering out duplicates based on underscore count.\"\"\"\n",
    "    video_files = [f for f in os.listdir(folder_path) if f.endswith('.mp4')]\n",
    "    \n",
    "    print(f\"\\nTotal videos in '{folder_path}' before filtering: {len(video_files)}\")\n",
    "\n",
    "    # Filter out videos where the filename contains the specified number of underscores\n",
    "    unique_videos = [os.path.join(folder_path, f) for f in video_files if f.count('_') != underscore_count]\n",
    "\n",
    "    print(f\"Total videos in '{folder_path}' after filtering: {len(unique_videos)}\")\n",
    "\n",
    "    return unique_videos\n",
    "\n",
    "# Get unique video files from both categories with respective underscore rules\n",
    "non_lifter_videos = get_unique_videos(non_shop_lifters_path, 4)  \n",
    "lifter_videos = get_unique_videos(shop_lifters_path, 3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\Cellula Internship cv\\\\Shop DataSet\\\\non shop lifters\\\\shop_lifter_n_102.mp4'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_lifter_videos[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_video(video_path, num_frames=100):\n",
    "    # Load video file\n",
    "    path = video_path.numpy().decode('utf-8')\n",
    "    cap = cv2.VideoCapture(path,apiPreference=cv2.CAP_ANY)\n",
    "    frames = []\n",
    "    \n",
    "    # Sample frames evenly from the video\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    sample_indices = np.linspace(50, total_frames - 1, num_frames, dtype=int)\n",
    "    \n",
    "    for i in sample_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            frame = np.expand_dims(frame, axis=-1)  # Add a channel dimension\n",
    "            frame = tf.image.resize(frame, [128, 128])\n",
    "            frame = tf.keras.applications.mobilenet_v2.preprocess_input(frame)\n",
    "            frames.append(frame)\n",
    "        else:\n",
    "            print('false')\n",
    "    \n",
    "    cap.release()\n",
    "    return tf.stack(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow dataset and save all videos in one tensor\n",
    "def create_dataset(video_paths, labels, batch_size=8):\n",
    "    all_videos = []  # List to store tensors for all videos\n",
    "    all_labels = []  # List to store corresponding labels\n",
    "    \n",
    "    # Iterate over video paths and labels to create video tensors\n",
    "    for video, label in zip(video_paths, labels):\n",
    "        video_tensor = tf.py_function(decode_video, [video], tf.float32)\n",
    "        all_videos.append(video_tensor)\n",
    "        all_labels.append(label)\n",
    "\n",
    "    # Stack all video tensors into one tensor\n",
    "    all_videos_tensor = tf.stack(all_videos)\n",
    "    all_labels_tensor = tf.convert_to_tensor(all_labels)\n",
    "\n",
    "    return all_videos_tensor, all_labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with a single video and its corresponding label\n",
    "non_litfer_tensor,non_labels = create_dataset(non_lifter_videos, ['0']*len(non_lifter_videos))  # Use 0 as the label for 'non'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([313, 100, 128, 128, 1]), TensorShape([313]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_litfer_tensor.shape, non_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with a single video and its corresponding label\n",
    "litfer_tensor,lifter_labels = create_dataset(lifter_videos, ['1']*len(lifter_videos))  # Use 0 as the label for 'non'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([324, 100, 128, 128, 1]), TensorShape([324]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litfer_tensor.shape,lifter_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.concat([non_litfer_tensor, litfer_tensor], axis=0)\n",
    "labels = tf.concat([non_labels, lifter_labels], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert TensorFlow tensors to NumPy arrays\n",
    "data_np = data.numpy()\n",
    "labels_np = labels.numpy()\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(data_np, labels_np, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(509, 100, 128, 128, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "((509, 100, 128, 128, 1), (509,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape,train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 100, 128, 128, 1), (128,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.shape,val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(val_data, val_labels, test_size=0.25, random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2,MobileNetV3Small\n",
    "from tensorflow.keras.layers import TimeDistributed, LSTM, Dense, Dropout\n",
    "\n",
    "def create_movinetv2_lstm_grayscale(num_frames, num_classes):\n",
    "    # Base MobileNetV2 with grayscale input\n",
    "    # Need to adjust the first layer to accept grayscale\n",
    "    inputs = tf.keras.layers.Input(shape=(128, 128, 1))\n",
    "    \n",
    "    # Convert grayscale to 3-channel by repeating the channel\n",
    "    # This allows us to still use the pretrained weights\n",
    "    x = tf.keras.layers.Conv2D(3, (1, 1), padding='same')(inputs)\n",
    "\n",
    "    # Base MobileNetV2 for feature extraction\n",
    "    base_model = MobileNetV2(\n",
    "        input_shape=(128, 128, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg'\n",
    "    )\n",
    "    \n",
    "    # Freeze early layers\n",
    "    for layer in base_model.layers[:100]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Create a model that can process a single grayscale frame\n",
    "    frame_processor = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(128, 128, 1)),\n",
    "        tf.keras.layers.Conv2D(3, (1, 1), padding='same'),  # Convert 1 channel to 3\n",
    "        base_model\n",
    "    ])\n",
    "    \n",
    "    # Create full model\n",
    "    input_layer = tf.keras.layers.Input(shape=(num_frames, 128, 128, 1))\n",
    "    \n",
    "    # Apply CNN to each frame independently\n",
    "    cnn_features = TimeDistributed(frame_processor)(input_layer)\n",
    "    \n",
    "    # Process temporal information\n",
    "    lstm = LSTM(512, return_sequences=False)(cnn_features)\n",
    "    x = Dropout(0.5)(lstm)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you've prepared these tensors already:\n",
    "# train_video_tensors: shape (num_samples, num_frames, height, width, channels)\n",
    "# train_labels: shape (num_samples,) containing class indices\n",
    "# val_video_tensors: shape (num_samples, num_frames, height, width, channels)\n",
    "# val_labels: shape (num_samples,) containing class indices\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Create MODEL (using the MoViNet+LSTM architecture recommended earlier)\n",
    "MODEL = create_movinetv2_lstm_grayscale(num_frames=100, num_classes=2)  # Adjust parameters as needed\n",
    "\n",
    "# Compile MODEL\n",
    "MODEL.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy','f1_score']\n",
    ")\n",
    "\n",
    "# Set up callbacks for training\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        'best_theft_detection_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=7,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model directly with tensors\n",
    "history = MODEL.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    validation_data=(val_data, val_labels),\n",
    "    epochs=30,\n",
    "    batch_size=8,  # Adjust based on your GPU memory\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model on validation data\n",
    "val_loss, val_accuracy = MODEL.evaluate(val_data, val_labels)\n",
    "print(f\"Validation Loss: {val_loss:.2f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
    "\n",
    "# Save the final model\n",
    "MODEL.save('final_theft_detection_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_video(video_path, model, num_frames=64):\n",
    "    # Process video using the same preprocessing as in training\n",
    "    video_tensor = decode_video(video_path, num_frames)\n",
    "    video_tensor = tf.expand_dims(video_tensor, axis=0)  # Add batch dimension\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = MODEL.predict(video_tensor)\n",
    "    class_index = tf.argmax(prediction[0]).numpy()\n",
    "    confidence = prediction[0][class_index].numpy()\n",
    "    \n",
    "    return class_index, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
